{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaec6dc-b532-4a72-b5bb-66de82c15be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a9510-149a-445b-8e47-069813efa245",
   "metadata": {},
   "source": [
    "# (SYNTHETIC DATA) Benchmarking vs. other paired models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b5c2e-da6d-4092-9648-6c8b1ddc595b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3719d85-2d03-4cdd-b644-2a9ee6dd952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = pickle.load(open('../data/comparisons_df.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1312a-443b-42ef-beb1-8b6d2807d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fdfc1-de8a-4f39-9b61-5d18a01ed34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = comparisons.copy()\n",
    "comparisons['image_l'] = comparisons['scene_i']\n",
    "comparisons['image_r'] = comparisons['scene_j']\n",
    "comparisons['Winner'] = -1\n",
    "comparisons['Loser'] = -1\n",
    "comparisons['Tie'] = 0\n",
    "\n",
    "for i, row in comparisons.iterrows():\n",
    "    l_item = row.image_l\n",
    "    r_item = row.image_r\n",
    "    \n",
    "    if row.score == 1:\n",
    "        comparisons.loc[i, 'Winner'] = r_item\n",
    "        comparisons.loc[i, 'Loser'] = l_item\n",
    "        comparisons.loc[i, 'Tie'] = 0\n",
    "    elif row.score == -1:\n",
    "        comparisons.loc[i, 'Winner'] = l_item\n",
    "        comparisons.loc[i, 'Loser'] = r_item\n",
    "        comparisons.loc[i, 'Tie'] = 0\n",
    "    elif row.score == 0:\n",
    "        comparisons.loc[i, 'Winner'] = r_item\n",
    "        comparisons.loc[i, 'Loser'] = l_item\n",
    "        comparisons.loc[i, 'Tie'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9b67b-d79e-4bcb-bd58-184d21b3cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_images = pd.unique(comparisons[['image_l', 'image_r']].values.ravel('K'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed9d1e-f4c0-4fda-9261-de14b5534b3a",
   "metadata": {},
   "source": [
    "Split data in Train, Validation & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4fedd-6917-4094-b9fb-494e7572c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(comparisons, test_size=0.2, random_state=seed)\n",
    "X_train, X_val  = train_test_split(X_train, test_size=0.13, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405acb75-1b0f-4757-844f-ba9b4d80c922",
   "metadata": {},
   "source": [
    "## TrueSkill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848113e-f6b6-4026-a93a-6f3f9ac2d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import trueskill as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d25820-10b6-4568-b49b-43710fe6823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(team1, team2):\n",
    "    BETA = ts.BETA\n",
    "    delta_mu = sum(r.mu for r in team1) - sum(r.mu for r in team2)\n",
    "    sum_sigma = sum(r.sigma ** 2 for r in itertools.chain(team1, team2))\n",
    "    size = len(team1) + len(team2)\n",
    "    denom = math.sqrt(size * (BETA * BETA) + sum_sigma)\n",
    "    ts_ = ts.global_env()\n",
    "    return ts_.cdf(delta_mu / denom), 1 - ts_.cdf(delta_mu / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7b252-7c90-42e6-9402-a549db68d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df):\n",
    "    accuracy = []\n",
    "    for i, row in df.iterrows():\n",
    "        p_win, p_los = compute_probabilities([scores[row.Winner]], [scores[row.Loser]])\n",
    "    \n",
    "        if row.score == -1 or row.score == 1:\n",
    "            accuracy.append(int(p_win > p_los))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d309a7b-50f5-4ae3-86c7-eb9d5d7509cc",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b148dca-b8ed-479b-86c8-c2d7dba3ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TrueSkill scores\n",
    "scores = {}\n",
    "\n",
    "for image in unique_images:\n",
    "    scores[image] = ts.Rating()\n",
    "\n",
    "# Compute scores based on comparisons\n",
    "for i, row in X_train.iterrows():\n",
    "    # Define the players in this round\n",
    "    player1 = scores[row['image_l']]\n",
    "    player2 = scores[row['image_r']]\n",
    "    \n",
    "    # Process match\n",
    "    if row['score'] == -1:\n",
    "        score = [0, 1]\n",
    "    elif row['score'] == 0:\n",
    "        score = [0, 0]\n",
    "    elif row['score'] == 1:\n",
    "        score = [1, 0]\n",
    "    \n",
    "    [player1], [player2] = ts.rate([[player1], [player2]], ranks=score)\n",
    "\n",
    "    # Update scores\n",
    "    scores[row['image_l']] = player1\n",
    "    scores[row['image_r']] = player2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5322d9-678a-4bb0-a80b-baacdc2d3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = compute_accuracy(X_train[X_train.score != 0])\n",
    "accuracy_test = compute_accuracy(X_test[X_test.score != 0])\n",
    "\n",
    "# Compile results\n",
    "result_ts = {\n",
    "    'model': 'trueskill',\n",
    "    'train_accuracy': np.mean(accuracy_train),\n",
    "    'test_accuracy': np.mean(accuracy_test),\n",
    "    'seed': seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837a3c4-d8e7-4489-9223-5541a0ea72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88fae2-cab5-4f4d-9c05-da9961b9cc55",
   "metadata": {},
   "source": [
    "## Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db303e78-d4af-4b4a-b569-c52c45eb5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalELo(object):\n",
    "    def __init__(self, k_factor, elo_width, starting_elo):\n",
    "        self.k_factor = k_factor\n",
    "        self.elo_width = elo_width\n",
    "        self.starting_elo = starting_elo\n",
    "        self.items = set()\n",
    "        self.items_elo = dict()\n",
    "        \n",
    "    def initialize_items(self, items):\n",
    "        \"\"\"Initialize the items available to `items`.\"\"\"\n",
    "        self.items = set(items)\n",
    "    \n",
    "    def initialize_elos(self, ):\n",
    "        \"\"\"Set the initial starting elo for all available items.\"\"\"\n",
    "        for item in self.items:\n",
    "            self.items_elo[item] = self.starting_elo\n",
    "    \n",
    "    def expected_result(self, elo_a, elo_b):\n",
    "        \"\"\"Expected probability of item with elo_a winning vs. item with elo_b.\"\"\"\n",
    "        \n",
    "        expect_a = 1.0/(1+10**((elo_b - elo_a)/self.elo_width))\n",
    "        return expect_a\n",
    "    \n",
    "    def update_elo(self, winner_elo, loser_elo, tie=False):\n",
    "        \"\"\"Update elo for the winning item and losing item.\"\"\"\n",
    "        \n",
    "        R = 1\n",
    "        if tie:\n",
    "            R = .5\n",
    "        \n",
    "        expected_win = self.expected_result(winner_elo, loser_elo)  \n",
    "        change_in_elo = self.k_factor * (R-expected_win)\n",
    "        \n",
    "        winner_elo += change_in_elo\n",
    "        loser_elo -= change_in_elo\n",
    "        return winner_elo, loser_elo\n",
    "    \n",
    "    def add_comparison(self, w_item, l_item, tie=False):\n",
    "        \"\"\"Process comparison between winning item and losing item.\"\"\"\n",
    "        current_winner_elo = self.items_elo[w_item]\n",
    "        current_loser_elo = self.items_elo[l_item]\n",
    "        \n",
    "        updated_winner_elo, updated_loser_elo = self.update_elo(current_winner_elo, current_loser_elo, tie=tie)\n",
    "        \n",
    "        self.items_elo[w_item] = updated_winner_elo\n",
    "        self.items_elo[l_item] = updated_loser_elo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408c7e3-4a0e-4cf6-82bc-bbb94d56f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(elo_a, elo_b, allow_ties=False):\n",
    "    \"\"\"\n",
    "    Expected probabilities of winning, drawing, or losing.\n",
    "    Reference for draws formula: `Mathematical Model of Ranking Accuracy and Popularity Promotion`\n",
    "    https://www.researchgate.net/publication/309662241_Mathematical_Model_of_Ranking_Accuracy_and_Popularity_Promotion\n",
    "    \"\"\"\n",
    "      \n",
    "    p_win = 1. / (1+10**((-elo_a + elo_b)/elo_width))\n",
    "    p_los = 1. / (1+10**((elo_a - elo_b)/elo_width))\n",
    "    \n",
    "    if allow_ties:\n",
    "        p_tie = (1 / (np.sqrt(2 * np.pi) * np.e)) * np.exp(-1 * (( (elo_a-elo_b)/(elo_width/2) )**2) / (2*np.e**2))\n",
    "        p_win = p_win - 0.5 * p_tie  \n",
    "        p_los = p_los - 0.5 * p_tie\n",
    "        \n",
    "        return  p_win, p_los, p_tie\n",
    "\n",
    "    return p_win, p_los\n",
    "    \n",
    "def compute_logloss(df):\n",
    "    log_loss = []\n",
    "    for i, row in df.iterrows():\n",
    "        # p_win, p_los, p_tie = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "        p_win, p_los = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "\n",
    "        if row.score == -1 or row.score == 1:\n",
    "            log_loss.append(np.log(p_win))\n",
    "        else:\n",
    "            log_loss.append(np.log(p_tie))\n",
    "    \n",
    "    return log_loss\n",
    "    \n",
    "def compute_accuracy(df):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        # p_win, p_los, p_tie = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "        p_win, p_los = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "\n",
    "        if row.score == -1 or row.score == 1:\n",
    "            accuracy.append(int(p_win > p_los))\n",
    "        else:\n",
    "            accuracy.append(int(p_win > p_los))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1710511-61e9-4880-9ab6-c4fdd5d484ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELO\n",
    "starting_elo = 1500\n",
    "elo_width = 400\n",
    "k_factor = 32\n",
    "\n",
    "elo = OriginalELo(k_factor=k_factor, \n",
    "                  elo_width=elo_width, \n",
    "                  starting_elo=starting_elo)\n",
    "elo.initialize_items(list(comparisons.Winner.values) + list(comparisons.Loser.values))\n",
    "elo.initialize_elos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6764ba9-c3cf-45ab-bda6-fb3ef5138687",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ee2fd-97ea-4c77-976d-73fd2b4edd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in X_train.iterrows():\n",
    "    w_item = row.Winner\n",
    "    l_item = row.Loser \n",
    "    tie = True if row.Tie else False\n",
    "    \n",
    "    elo.add_comparison(w_item, l_item, tie=tie)    \n",
    "scores = []\n",
    "\n",
    "for item, item_elo in elo.items_elo.items():\n",
    "    scores.append({\n",
    "        'score': item_elo,\n",
    "        'image': item,\n",
    "    })\n",
    "scores_df = pd.DataFrame(scores).set_index('image', drop=False)\n",
    "\n",
    "accuracy_train = compute_accuracy(X_train[X_train.score != 0])\n",
    "accuracy_test = compute_accuracy(X_test[X_test.score != 0])\n",
    "\n",
    "# Compile results\n",
    "result_elo = {\n",
    "    'model': 'elo',\n",
    "    'train_accuracy': np.mean(accuracy_train),\n",
    "    'test_accuracy': np.mean(accuracy_test),\n",
    "    'seed': seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757da679-5640-4179-b633-b51bdd42d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_elo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601db9b-a31d-47e9-a6f0-c1f293ae5cd3",
   "metadata": {},
   "source": [
    "## Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042d520-7387-4ad0-a8cc-89a48966238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kickscore as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8141db-8dcd-475b-a323-b9c752fb269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df, t_):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            p_win, p_tie, p_los = model.probabilities([row.Winner], [row.Loser], t=t_)\n",
    "\n",
    "            if row.score == -1 or row.score == 1:\n",
    "                if p_win > p_los and p_win > p_tie:\n",
    "                    accuracy.append(1) \n",
    "                else:\n",
    "                    accuracy.append(0) \n",
    "            else:  \n",
    "                if p_tie > p_los and p_tie > p_win:\n",
    "                    accuracy.append(1) \n",
    "                else:\n",
    "                    accuracy.append(0) \n",
    "        except KeyError:\n",
    "            continue\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74138105-dac9-48ec-ad5d-31729b0d399d",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261caf61-4340-4fc6-835e-ddf5ac19c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = set()\n",
    "observations = list()\n",
    "\n",
    "for i, row in X_train.reset_index().iterrows():\n",
    "    t = i\n",
    "    images.add(row.image_l)\n",
    "    images.add(row.image_r)\n",
    "    \n",
    "    if row.score == -1:\n",
    "        observations.append({\n",
    "                'winners': [row.image_l],\n",
    "                'losers': [row.image_r],\n",
    "                #'tie': False,\n",
    "                't': t,\n",
    "            })\n",
    "    if row.score == 0:\n",
    "        observations.append({\n",
    "                'winners': [row.image_l],\n",
    "                'losers': [row.image_r],\n",
    "                'tie': True,\n",
    "                't': t,\n",
    "            })\n",
    "    if row.score == 1:\n",
    "        observations.append({\n",
    "                'winners': [row.image_r],\n",
    "                'losers': [row.image_l],\n",
    "                #'tie': False,\n",
    "                't': t,\n",
    "            })\n",
    "\n",
    "model = ks.TernaryModel(margin=0.2)\n",
    "kernel = (ks.kernel.Constant(var=0.03))\n",
    "for image in images:\n",
    "    model.add_item(image, kernel=kernel)\n",
    "for obs in observations:\n",
    "    model.observe(**obs)\n",
    "\n",
    "converged = model.fit()\n",
    "if converged:\n",
    "    print(\"Model has converged.\")\n",
    "\n",
    "ts = [comparisons.shape[0] ]  # Point in time at which you want to make the prediction.\n",
    "res = dict()  # Contains predicted score.\n",
    "\n",
    "scores = []\n",
    "for name, item in model.item.items():\n",
    "    means, var = item.predict(ts)\n",
    "    scores += [[name, means[0], var[0]]]\n",
    "\n",
    "scores = pd.DataFrame(scores, columns=['image', 'score', 'var'])\n",
    "scores_df = scores.sort_values(by='image').reset_index(drop=True)\n",
    "\n",
    "scores_df = scores_df.set_index('image', drop=False)\n",
    "scores_df.index.name = None\n",
    "\n",
    "accuracy_train = compute_accuracy(X_train[X_train.score != 0], comparisons.shape[0])\n",
    "accuracy_test = compute_accuracy(X_test[X_test.score != 0], comparisons.shape[0])\n",
    "\n",
    "# Compile results\n",
    "result_gp = {\n",
    "    'model': 'gaussian_process',\n",
    "    'train_accuracy': np.mean(accuracy_train),\n",
    "    'test_accuracy': np.mean(accuracy_test),\n",
    "    'seed': seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881095d-213c-4ee2-8c15-edc8416699d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ee171-2ba0-48eb-98c7-af8c84c5646f",
   "metadata": {},
   "source": [
    "## Rank Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c1eed-b434-487a-8b45-af26d7dbda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157b104-a06a-4936-bac4-656c6574eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df, params):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row.score == -1:\n",
    "            p_win, p_los = choix.probabilities([int(row.image_l), int(row.image_r)], params)\n",
    "        elif row.score == 1:\n",
    "            p_win, p_los = choix.probabilities([int(row.image_r), int(row.image_l)], params)\n",
    "\n",
    "        if row.score == -1 or row.score == 1:\n",
    "            if p_win > p_los:\n",
    "                accuracy.append(1) \n",
    "            else:\n",
    "                accuracy.append(0)  \n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530306e3-e242-4ef1-96bf-4b131965abd2",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cc6cf-02ab-465e-814e-6a10445bfc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_ = comparisons.copy()\n",
    "# RANK CENTRALITY\n",
    "n_items = len(pd.unique(comparisons_[['image_l', 'image_r']].values.ravel('K')))\n",
    "images = pd.unique(comparisons_[['image_l', 'image_r']].values.ravel('K'))\n",
    "images_dict = {}\n",
    "images_dict_rev = {}\n",
    "for i, image_id in enumerate(images):\n",
    "    images_dict[image_id] = i\n",
    "    images_dict_rev[i] = image_id\n",
    "comparisons_=comparisons_.replace({\"image_l\": images_dict})\n",
    "comparisons_=comparisons_.replace({\"image_r\": images_dict})\n",
    "comparisons_=comparisons_.replace({\"Winner\": images_dict})\n",
    "comparisons_=comparisons_.replace({\"Loser\": images_dict})\n",
    "\n",
    "# Split data in Train, Validation & Test\n",
    "X_train, X_test = train_test_split(comparisons_, test_size=0.2, random_state=seed)\n",
    "X_train, X_val  = train_test_split(X_train, test_size=0.13, random_state=seed)\n",
    "\n",
    "data = []\n",
    "for i, row in X_train.iterrows():\n",
    "    if not row.Tie:\n",
    "        data.append((int(row.Winner), int(row.Loser)))\n",
    "\n",
    "    if row.Tie:\n",
    "        data.append((row.Winner, row.Loser))\n",
    "        data.append((row.Loser, row.Winner))\n",
    "\n",
    "params_rc = choix.rank_centrality(n_items, data, alpha=1e-4)\n",
    "\n",
    "accuracy_train = compute_accuracy(X_train[X_train.score != 0], params_rc)\n",
    "accuracy_test = compute_accuracy(X_test[X_test.score != 0], params_rc)\n",
    "\n",
    "# Compile results\n",
    "result_rc = {\n",
    "    'model': 'rank_centrality',\n",
    "    'train_accuracy': np.mean(accuracy_train),\n",
    "    'test_accuracy': np.mean(accuracy_test),\n",
    "    'seed': seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb76103-f3be-4242-8c23-37b7a75c8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d61207-1d96-446e-9194-c7351281dbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bf4a214-b4d7-43da-a544-5f6dc626c5cf",
   "metadata": {},
   "source": [
    "# (REAL DATA) Benchmarking vs. other paired models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6dd657-14a4-4e02-be8a-cfa3a2c8c071",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6c618-e7a9-4953-9817-3e0f1e4e169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = pickle.load(open('../data/berlin_comparisons_df.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e800da-1420-4d4d-ac66-e41c4ad41f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402918f-f352-4770-a03f-0bc005693c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = comparisons.copy()\n",
    "comparisons['Winner'] = -1\n",
    "comparisons['Loser'] = -1\n",
    "comparisons['Tie'] = 0\n",
    "\n",
    "for i, row in comparisons.iterrows():\n",
    "    l_item = row.image_l\n",
    "    r_item = row.image_r\n",
    "    \n",
    "    if row.score == 1:\n",
    "        comparisons.loc[i, 'Winner'] = r_item\n",
    "        comparisons.loc[i, 'Loser'] = l_item\n",
    "        comparisons.loc[i, 'Tie'] = 0\n",
    "    elif row.score == -1:\n",
    "        comparisons.loc[i, 'Winner'] = l_item\n",
    "        comparisons.loc[i, 'Loser'] = r_item\n",
    "        comparisons.loc[i, 'Tie'] = 0\n",
    "    elif row.score == 0:\n",
    "        comparisons.loc[i, 'Winner'] = r_item\n",
    "        comparisons.loc[i, 'Loser'] = l_item\n",
    "        comparisons.loc[i, 'Tie'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b1ca4-1ad9-4d96-92a7-111b48af8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_images = pd.unique(comparisons[['image_l', 'image_r']].values.ravel('K'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e4b8e-4f14-4758-9de0-e82641b8e947",
   "metadata": {},
   "source": [
    "Split data in Train, Validation & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1a4b8-1a0c-40ce-acaa-a46f77b05104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(comparisons, test_size=0.2, random_state=seed)\n",
    "X_train, X_val  = train_test_split(X_train, test_size=0.13, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a788a-31df-4f33-9b6b-b45bfc3471df",
   "metadata": {},
   "source": [
    "## TrueSkill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39450f88-cf82-4257-970e-071a7c0542d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import trueskill as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e923f-9fd5-42b0-b5a7-8e5f0c831cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(team1, team2):\n",
    "    BETA = ts.BETA\n",
    "    delta_mu = sum(r.mu for r in team1) - sum(r.mu for r in team2)\n",
    "    sum_sigma = sum(r.sigma ** 2 for r in itertools.chain(team1, team2))\n",
    "    size = len(team1) + len(team2)\n",
    "    denom = math.sqrt(size * (BETA * BETA) + sum_sigma)\n",
    "    ts_ = ts.global_env()\n",
    "    return ts_.cdf(delta_mu / denom), 1 - ts_.cdf(delta_mu / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb753feb-1462-48dc-9caa-5f875eb2046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df):\n",
    "    accuracy = []\n",
    "    for i, row in df.iterrows():\n",
    "        p_win, p_los = compute_probabilities([scores[row.Winner]], [scores[row.Loser]])\n",
    "    \n",
    "        if row.score == -1 or row.score == 1:\n",
    "            accuracy.append(int(p_win > p_los))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64095c5-893f-4d23-894e-5f7263825e30",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848a5c0-f676-4c47-ae8e-bcb68b7bff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TrueSkill scores\n",
    "scores = {}\n",
    "\n",
    "for image in unique_images:\n",
    "    scores[image] = ts.Rating()\n",
    "\n",
    "# Compute scores based on comparisons\n",
    "for i, row in X_train.iterrows():\n",
    "    # Define the players in this round\n",
    "    player1 = scores[row['image_l']]\n",
    "    player2 = scores[row['image_r']]\n",
    "    \n",
    "    # Process match\n",
    "    if row['score'] == -1:\n",
    "        score = [0, 1]\n",
    "    elif row['score'] == 0:\n",
    "        score = [0, 0]\n",
    "    elif row['score'] == 1:\n",
    "        score = [1, 0]\n",
    "    \n",
    "    [player1], [player2] = ts.rate([[player1], [player2]], ranks=score)\n",
    "\n",
    "    # Update scores\n",
    "    scores[row['image_l']] = player1\n",
    "    scores[row['image_r']] = player2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a507d2-4160-4764-8b69-929d4fd99316",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = compute_accuracy(X_train[X_train.score != 0])\n",
    "accuracy_test = compute_accuracy(X_test[X_test.score != 0])\n",
    "\n",
    "# Compile results\n",
    "result_ts = {\n",
    "    'model': 'trueskill',\n",
    "    'train_accuracy': np.mean(accuracy_train),\n",
    "    'test_accuracy': np.mean(accuracy_test),\n",
    "    'seed': seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0aa7a-9a9f-4f1c-9fe0-8cbde49a23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0668d-d9ef-439a-ad5f-4a81b484d980",
   "metadata": {},
   "source": [
    "## Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a681b-b73b-42f1-b811-058f98a7190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalELo(object):\n",
    "    def __init__(self, k_factor, elo_width, starting_elo):\n",
    "        self.k_factor = k_factor\n",
    "        self.elo_width = elo_width\n",
    "        self.starting_elo = starting_elo\n",
    "        self.items = set()\n",
    "        self.items_elo = dict()\n",
    "        \n",
    "    def initialize_items(self, items):\n",
    "        \"\"\"Initialize the items available to `items`.\"\"\"\n",
    "        self.items = set(items)\n",
    "    \n",
    "    def initialize_elos(self, ):\n",
    "        \"\"\"Set the initial starting elo for all available items.\"\"\"\n",
    "        for item in self.items:\n",
    "            self.items_elo[item] = self.starting_elo\n",
    "    \n",
    "    def expected_result(self, elo_a, elo_b):\n",
    "        \"\"\"Expected probability of item with elo_a winning vs. item with elo_b.\"\"\"\n",
    "        \n",
    "        expect_a = 1.0/(1+10**((elo_b - elo_a)/self.elo_width))\n",
    "        return expect_a\n",
    "    \n",
    "    def update_elo(self, winner_elo, loser_elo, tie=False):\n",
    "        \"\"\"Update elo for the winning item and losing item.\"\"\"\n",
    "        \n",
    "        R = 1\n",
    "        if tie:\n",
    "            R = .5\n",
    "        \n",
    "        expected_win = self.expected_result(winner_elo, loser_elo)  \n",
    "        change_in_elo = self.k_factor * (R-expected_win)\n",
    "        \n",
    "        winner_elo += change_in_elo\n",
    "        loser_elo -= change_in_elo\n",
    "        return winner_elo, loser_elo\n",
    "    \n",
    "    def add_comparison(self, w_item, l_item, tie=False):\n",
    "        \"\"\"Process comparison between winning item and losing item.\"\"\"\n",
    "        current_winner_elo = self.items_elo[w_item]\n",
    "        current_loser_elo = self.items_elo[l_item]\n",
    "        \n",
    "        updated_winner_elo, updated_loser_elo = self.update_elo(current_winner_elo, current_loser_elo, tie=tie)\n",
    "        \n",
    "        self.items_elo[w_item] = updated_winner_elo\n",
    "        self.items_elo[l_item] = updated_loser_elo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fe8b1-d59d-418e-80b0-f7cd94a21c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(elo_a, elo_b, allow_ties=False):\n",
    "    \"\"\"\n",
    "    Expected probabilities of winning, drawing, or losing.\n",
    "    Reference for draws formula: `Mathematical Model of Ranking Accuracy and Popularity Promotion`\n",
    "    https://www.researchgate.net/publication/309662241_Mathematical_Model_of_Ranking_Accuracy_and_Popularity_Promotion\n",
    "    \"\"\"\n",
    "      \n",
    "    p_win = 1. / (1+10**((-elo_a + elo_b)/elo_width))\n",
    "    p_los = 1. / (1+10**((elo_a - elo_b)/elo_width))\n",
    "    \n",
    "    if allow_ties:\n",
    "        p_tie = (1 / (np.sqrt(2 * np.pi) * np.e)) * np.exp(-1 * (( (elo_a-elo_b)/(elo_width/2) )**2) / (2*np.e**2))\n",
    "        p_win = p_win - 0.5 * p_tie  \n",
    "        p_los = p_los - 0.5 * p_tie\n",
    "        \n",
    "        return  p_win, p_los, p_tie\n",
    "\n",
    "    return p_win, p_los\n",
    "    \n",
    "def compute_logloss(df):\n",
    "    log_loss = []\n",
    "    for i, row in df.iterrows():\n",
    "        # p_win, p_los, p_tie = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "        p_win, p_los = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "\n",
    "        if row.score == -1 or row.score == 1:\n",
    "            log_loss.append(np.log(p_win))\n",
    "        else:\n",
    "            log_loss.append(np.log(p_tie))\n",
    "    \n",
    "    return log_loss\n",
    "    \n",
    "def compute_accuracy(df):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        # p_win, p_los, p_tie = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "        p_win, p_los = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "\n",
    "        if row.score == -1 or row.score == 1:\n",
    "            accuracy.append(int(p_win > p_los))\n",
    "        else:\n",
    "            accuracy.append(int(p_win > p_los))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54640c60-aac0-4712-b556-63a7f87fe4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELO\n",
    "starting_elo = 1500\n",
    "elo_width = 400\n",
    "k_factor = 32\n",
    "\n",
    "elo = OriginalELo(k_factor=k_factor, \n",
    "                  elo_width=elo_width, \n",
    "                  starting_elo=starting_elo)\n",
    "elo.initialize_items(list(comparisons.Winner.values) + list(comparisons.Loser.values))\n",
    "elo.initialize_elos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba69dff-5ace-414b-850a-b7f368081881",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00570c-c68e-4a12-85d2-e1a37b5e6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in X_train.iterrows():\n",
    "    w_item = row.Winner\n",
    "    l_item = row.Loser \n",
    "    tie = True if row.Tie else False\n",
    "    \n",
    "    elo.add_comparison(w_item, l_item, tie=tie)    \n",
    "scores = []\n",
    "\n",
    "for item, item_elo in elo.items_elo.items():\n",
    "    scores.append({\n",
    "        'score': item_elo,\n",
    "        'image': item,\n",
    "    })\n",
    "scores_df = pd.DataFrame(scores).set_index('image', drop=False)\n",
    "\n",
    "accuracy_train = compute_accuracy(X_train[X_train.score != 0])\n",
    "accuracy_test = compute_accuracy(X_test[X_test.score != 0])\n",
    "\n",
    "# Compile results\n",
    "result_elo = {\n",
    "    'model': 'elo',\n",
    "    'train_accuracy': np.mean(accuracy_train),\n",
    "    'test_accuracy': np.mean(accuracy_test),\n",
    "    'seed': seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10a0e6-0219-4115-aa92-c1c54c506a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_elo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4b9e8-fb0a-4a55-ba0f-299f0e3415c6",
   "metadata": {},
   "source": [
    "## Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc79902-d9b8-4cb0-83dc-0e7852e0b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kickscore as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437220f1-b567-453d-96a9-848bb656a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df, t_):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            p_win, p_tie, p_los = model.probabilities([row.Winner], [row.Loser], t=t_)\n",
    "\n",
    "            if row.score == -1 or row.score == 1:\n",
    "                if p_win > p_los and p_win > p_tie:\n",
    "                    accuracy.append(1) \n",
    "                else:\n",
    "                    accuracy.append(0) \n",
    "            else:  \n",
    "                if p_tie > p_los and p_tie > p_win:\n",
    "                    accuracy.append(1) \n",
    "                else:\n",
    "                    accuracy.append(0) \n",
    "        except KeyError:\n",
    "            continue\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6fdf9-1fc3-4bce-a412-8346a2896403",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e3ecc-61be-4a01-93d3-bc1a2e0dd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = set()\n",
    "observations = list()\n",
    "\n",
    "for i, row in X_train.reset_index().iterrows():\n",
    "    t = i\n",
    "    images.add(row.image_l)\n",
    "    images.add(row.image_r)\n",
    "    \n",
    "    if row.score == -1:\n",
    "        observations.append({\n",
    "                'winners': [row.image_l],\n",
    "                'losers': [row.image_r],\n",
    "                #'tie': False,\n",
    "                't': t,\n",
    "            })\n",
    "    if row.score == 0:\n",
    "        observations.append({\n",
    "                'winners': [row.image_l],\n",
    "                'losers': [row.image_r],\n",
    "                'tie': True,\n",
    "                't': t,\n",
    "            })\n",
    "    if row.score == 1:\n",
    "        observations.append({\n",
    "                'winners': [row.image_r],\n",
    "                'losers': [row.image_l],\n",
    "                #'tie': False,\n",
    "                't': t,\n",
    "            })\n",
    "\n",
    "model = ks.TernaryModel(margin=0.2)\n",
    "kernel = (ks.kernel.Constant(var=0.03))\n",
    "for image in images:\n",
    "    model.add_item(image, kernel=kernel)\n",
    "for obs in observations:\n",
    "    model.observe(**obs)\n",
    "\n",
    "converged = model.fit()\n",
    "if converged:\n",
    "    print(\"Model has converged.\")\n",
    "\n",
    "ts = [comparisons.shape[0] ]  # Point in time at which you want to make the prediction.\n",
    "res = dict()  # Contains predicted score.\n",
    "\n",
    "scores = []\n",
    "for name, item in model.item.items():\n",
    "    means, var = item.predict(ts)\n",
    "    scores += [[name, means[0], var[0]]]\n",
    "\n",
    "scores = pd.DataFrame(scores, columns=['image', 'score', 'var'])\n",
    "scores_df = scores.sort_values(by='image').reset_index(drop=True)\n",
    "\n",
    "scores_df = scores_df.set_index('image', drop=False)\n",
    "scores_df.index.name = None\n",
    "\n",
    "accuracy_train = compute_accuracy(X_train[X_train.score != 0], comparisons.shape[0])\n",
    "accuracy_test = compute_accuracy(X_test[X_test.score != 0], comparisons.shape[0])\n",
    "\n",
    "# Compile results\n",
    "result_gp = {\n",
    "    'model': 'gaussian_process',\n",
    "    'train_accuracy': np.mean(accuracy_train),\n",
    "    'test_accuracy': np.mean(accuracy_test),\n",
    "    'seed': seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923d962-5b6c-4dfe-9104-b71f519a07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cfb117-bc3c-42be-b810-a87ffddc7823",
   "metadata": {},
   "source": [
    "## Rank Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75872c88-3957-4808-b804-d589a1e82741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c4742-5ce6-4f45-b9c4-288d6dc42b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df, params):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row.score == -1:\n",
    "            p_win, p_los = choix.probabilities([int(row.image_l), int(row.image_r)], params)\n",
    "        elif row.score == 1:\n",
    "            p_win, p_los = choix.probabilities([int(row.image_r), int(row.image_l)], params)\n",
    "\n",
    "        if row.score == -1 or row.score == 1:\n",
    "            if p_win > p_los:\n",
    "                accuracy.append(1) \n",
    "            else:\n",
    "                accuracy.append(0)  \n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1caac93-12cc-4406-bff9-155c17757853",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa202c1-87b2-43ef-96b3-1b8c430c2b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_ = comparisons.copy()\n",
    "# RANK CENTRALITY\n",
    "n_items = len(pd.unique(comparisons_[['image_l', 'image_r']].values.ravel('K')))\n",
    "images = pd.unique(comparisons_[['image_l', 'image_r']].values.ravel('K'))\n",
    "images_dict = {}\n",
    "images_dict_rev = {}\n",
    "for i, image_id in enumerate(images):\n",
    "    images_dict[image_id] = i\n",
    "    images_dict_rev[i] = image_id\n",
    "comparisons_=comparisons_.replace({\"image_l\": images_dict})\n",
    "comparisons_=comparisons_.replace({\"image_r\": images_dict})\n",
    "comparisons_=comparisons_.replace({\"Winner\": images_dict})\n",
    "comparisons_=comparisons_.replace({\"Loser\": images_dict})\n",
    "\n",
    "# Split data in Train, Validation & Test\n",
    "X_train, X_test = train_test_split(comparisons_, test_size=0.2, random_state=seed)\n",
    "X_train, X_val  = train_test_split(X_train, test_size=0.13, random_state=seed)\n",
    "\n",
    "data = []\n",
    "for i, row in X_train.iterrows():\n",
    "    if not row.Tie:\n",
    "        data.append((int(row.Winner), int(row.Loser)))\n",
    "\n",
    "    if row.Tie:\n",
    "        data.append((row.Winner, row.Loser))\n",
    "        data.append((row.Loser, row.Winner))\n",
    "\n",
    "params_rc = choix.rank_centrality(n_items, data, alpha=1e-4)\n",
    "\n",
    "accuracy_train = compute_accuracy(X_train[X_train.score != 0], params_rc)\n",
    "accuracy_test = compute_accuracy(X_test[X_test.score != 0], params_rc)\n",
    "\n",
    "# Compile results\n",
    "result_rc = {\n",
    "    'model': 'rank_centrality',\n",
    "    'train_accuracy': np.mean(accuracy_train),\n",
    "    'test_accuracy': np.mean(accuracy_test),\n",
    "    'seed': seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494fa920-68a3-4f65-a3d5-0bfc4ca6b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
