{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3188b-5237-4130-a517-77f63ce95e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebaf03-2bca-4e7e-9213-3bd141956099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b99668-9c08-4bdb-9057-845e19626607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "#rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "rc('font',**{'family':'serif',\n",
    "             'serif':['Times New Roman'],\n",
    "             'size': 12,\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763d7df-c74f-479a-8c58-631a6466c030",
   "metadata": {},
   "source": [
    "# PCS-Net vs. Paired Models, with varying number of avg. pairwise comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b555e-f2a3-4d8e-acd5-36e6a45f1e33",
   "metadata": {},
   "source": [
    "## PCS-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe9094-7d71-4295-9fca-091413d65a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_accuracy(df_, margin=0):\n",
    "    # Split in non-ties and ties\n",
    "    df_nonties = df_[df_.label_r != 0]\n",
    "    df_ties = df_[df_.label_r == 0]\n",
    "\n",
    "    # Split non ties per their outcome (left and right)\n",
    "    df_nonties_left = df_nonties[(df_nonties.label_r == -1)]\n",
    "    df_nonties_right = df_nonties[(df_nonties.label_r == 1)]\n",
    "\n",
    "    # Non-ties accuracy\n",
    "    correct_left = ((df_nonties.label_r == -1) & (df_nonties.rank_left - df_nonties.rank_right > margin)).sum()\n",
    "    correct_right = ((df_nonties.label_r == 1) & (df_nonties.rank_right - df_nonties.rank_left > margin)).sum()\n",
    "\n",
    "    nontie_left_accuracy = correct_left / (df_nonties.label_r == -1).sum()\n",
    "    nontie_right_accuracy = correct_right / (df_nonties.label_r == 1).sum()\n",
    "    nontie_accuracy = (correct_left + correct_right ) / df_nonties.shape[0]\n",
    "    \n",
    "    # Ties accuracy\n",
    "    tie_accuracy = (abs(df_ties.rank_left - df_ties.rank_right) < margin).sum() / df_ties.shape[0]\n",
    "\n",
    "    # Overall accuracy\n",
    "    overall_accuracy = X_test[((df_.label_r == -1) & (df_.rank_left - df_.rank_right > margin)) |\n",
    "                              ((df_.label_r ==  1) & (df_.rank_right - df_.rank_left > margin)) |\n",
    "                              ((df_.label_r ==  0) & (abs(df_.rank_left - df_.rank_right) < margin))].shape[0] / df_.shape[0]\n",
    "    \n",
    "    return nontie_left_accuracy, nontie_right_accuracy, nontie_accuracy, tie_accuracy, overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaac611-bd51-4155-9251-3aaf42bb7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_accuracy_nomargin(df_,):\n",
    "    # Split in non-ties and ties\n",
    "    df_nonties = df_[df_.label_r != 0]\n",
    "\n",
    "    # Split non ties per their outcome (left and right)\n",
    "    df_nonties_left = df_nonties[(df_nonties.label_r == -1)]\n",
    "    df_nonties_right = df_nonties[(df_nonties.label_r == 1)]\n",
    "\n",
    "    # Non-ties accuracy\n",
    "    correct_left = ((df_nonties.label_r == -1) & (df_nonties.rank_left - df_nonties.rank_right > 0)).sum()\n",
    "    correct_right = ((df_nonties.label_r == 1) & (df_nonties.rank_right - df_nonties.rank_left > 0)).sum()\n",
    "\n",
    "    nontie_left_accuracy = correct_left / (df_nonties.label_r == -1).sum()\n",
    "    nontie_right_accuracy = correct_right / (df_nonties.label_r == 1).sum()\n",
    "    nontie_accuracy = (correct_left + correct_right ) / df_nonties.shape[0]\n",
    "\n",
    "    return nontie_left_accuracy, nontie_right_accuracy, nontie_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b642d78-1fc5-4ab0-a843-52522f41b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_distance(df_):\n",
    "    # Split in non-ties and ties\n",
    "    df_nonties = df_[df_.label_r != 0]\n",
    "    df_ties = df_[df_.label_r == 0]\n",
    "\n",
    "    # Distance between non-ties\n",
    "    avg_dist_nonties = abs(df_nonties.rank_left - df_nonties.rank_right).mean()\n",
    "    \n",
    "    # Distance between ties\n",
    "    avg_dist_ties = abs(df_ties.rank_left - df_ties.rank_right).mean()\n",
    "    \n",
    "    return avg_dist_nonties, avg_dist_ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f023dd-8aaf-4fe7-9ba9-14cb2c16986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy(df_):\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "        \n",
    "    col1_values = df_['logits_l'].values\n",
    "    col2_values = df_['logits_0'].values\n",
    "    col3_values = df_['logits_r'].values\n",
    "        \n",
    "    probabilities = np.apply_along_axis(softmax, axis=1, arr=np.column_stack((col1_values, col2_values, col3_values)))\n",
    "    max_indices = np.argmax(probabilities, axis=1)\n",
    "    # Convert the probabilities back to a DataFrame with appropriate column names\n",
    "    softmax_df = pd.DataFrame(probabilities, columns=['softmax_logit_l', 'softmax_logit_0', 'softmax_logit_r'])\n",
    "    max_index_df = pd.DataFrame({'class_predicted': max_indices})\n",
    "    # Concatenate the new DataFrame with the original DataFrame if needed\n",
    "    result_df = pd.concat([df.reset_index(drop=True), softmax_df, max_index_df], axis=1,)\n",
    "\n",
    "    all_accuracy = (result_df.class_predicted == result_df.label_c).sum() / result_df.shape[0]\n",
    "    tie_accuracy = (result_df[result_df.label_c == 1].class_predicted == result_df[result_df.label_c == 1].label_c).sum() / result_df.shape[0]\n",
    "    nontie_accuracy = (result_df[result_df.label_c != 1].class_predicted == result_df[result_df.label_c != 1].label_c).sum() / result_df.shape[0]\n",
    "    \n",
    "    return all_accuracy, tie_accuracy, nontie_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c936a-f55e-4fec-9e83-ecbcf5c11da2",
   "metadata": {},
   "source": [
    "### Available results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac691b-2418-4df7-9b11-f7bec53754e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = glob('../outputs/saved/synthetic-*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f4f60-a217-4f72-b30d-389344ed461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = glob('../data/comparisons_synthetic_pc*.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fcabb4-c79f-43d4-b057-ea8ae14af275",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(model_data[0]).individual_id.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f489b-275c-4ac3-9945-f3fb18856f78",
   "metadata": {},
   "source": [
    "### Process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67609ec3-fabc-41db-858d-b441dcb6b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_results = []\n",
    "for model_result in model_results:\n",
    "    df = pd.read_pickle(model_result)\n",
    "    avg_pc = float(os.path.basename(model_result).replace('synthetic-', '').replace('.pt_results.pkl', ''))\n",
    "    orig\n",
    "    seed = 30\n",
    "    margin_= 1\n",
    "    print('Avg. Pairwise Comparisons:', avg_pc, '-->', model_result)\n",
    "\n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=seed)\n",
    "    X_train, X_val  = train_test_split(X_train, test_size=0.13, random_state=seed)\n",
    "    # print('\\tTrain:     ', X_train.shape)\n",
    "    # print('\\tValidation:', X_val.shape) \n",
    "    # print('\\tTest:      ', X_test.shape)\n",
    "\n",
    "    # Ranking sub-network\n",
    "    nontie_left_accuracy, nontie_right_accuracy, nontie_accuracy, tie_accuracy, overall_accuracy = compute_ranking_accuracy(X_test, margin=margin_)\n",
    "\n",
    "    # Ranking sub-network, without any margin on accuracy\n",
    "    nontie_left_accuracy_nomargin, nontie_right_accuracy_nomargin, nontie_accuracy_nomargin = compute_ranking_accuracy_nomargin(X_test)\n",
    "\n",
    "    # Classification sub-network\n",
    "    c_all_accuracy, c_tie_accuracy, c_nontie_accuracy = compute_classification_accuracy(X_test)\n",
    "\n",
    "    # Rank difference\n",
    "    avg_dist_nonties, avg_dist_ties = compute_ranking_distance(X_test)\n",
    "    \n",
    "    # Compile results\n",
    "    result = {\n",
    "        'avg_pc': avg_pc,\n",
    "        'seed': seed,\n",
    "        # Ranking, with margins\n",
    "        'ranking_acc': overall_accuracy,\n",
    "        'ranking_acc_nonties': nontie_accuracy,\n",
    "        'ranking_acc_ties': tie_accuracy,\n",
    "        'ranking_acc_left': nontie_left_accuracy,\n",
    "        'ranking_acc_right': nontie_right_accuracy,\n",
    "        # Ranking, without margins\n",
    "        'ranking_acc_nonties_nomargin': nontie_accuracy_nomargin,\n",
    "        'ranking_acc_left_nomargin': nontie_left_accuracy_nomargin,\n",
    "        'ranking_acc_right_nomargin': nontie_right_accuracy_nomargin,\n",
    "        # Classification\n",
    "        'classification_acc': c_all_accuracy,\n",
    "        'classification_acc_nonties': c_nontie_accuracy,\n",
    "        'classification_acc_ties': c_tie_accuracy,\n",
    "        # Rank difference\n",
    "        'avg_dist_nonties': avg_dist_nonties,\n",
    "        'avg_dist_ties': avg_dist_ties,\n",
    "    }\n",
    "    compiled_results.append(result)\n",
    "    \n",
    "results_df = pd.DataFrame(compiled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c01c0c-a129-44e5-99e8-e96ffb3036e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_pcs = results_df.sort_values(by=['avg_pc'])\n",
    "results_df_pcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2d830-c08e-4161-8783-66b5fce4c08d",
   "metadata": {},
   "source": [
    "## TrueSkill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d3500-f61e-4db2-8039-92d7bcc42b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import trueskill as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f268cc-d778-4c66-bf23-cfdaca4f8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(team1, team2):\n",
    "    BETA = ts.BETA\n",
    "    delta_mu = sum(r.mu for r in team1) - sum(r.mu for r in team2)\n",
    "    sum_sigma = sum(r.sigma ** 2 for r in itertools.chain(team1, team2))\n",
    "    size = len(team1) + len(team2)\n",
    "    denom = math.sqrt(size * (BETA * BETA) + sum_sigma)\n",
    "    ts_ = ts.global_env()\n",
    "    return ts_.cdf(delta_mu / denom), 1 - ts_.cdf(delta_mu / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf2cde-d896-4490-9796-f50935141a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        p_win, p_los = compute_probabilities([scores[row.Winner]], [scores[row.Loser]])\n",
    "    \n",
    "        if row.score == -1 or row.score == 1:\n",
    "            accuracy.append(int(p_win > p_los))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6bac7-3f7a-4649-bd6e-d06b2a742f16",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e50d2c-bbcd-4da2-a3f5-49a353d186b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = glob('../data/comparisons_synthetic_pc*.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e8152-fff6-42fc-bc09-12a96c23e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for df_name  in model_results:\n",
    "    comparisons_ = pickle.load(open(df_name, 'rb'))\n",
    "    avg_pc = os.path.basename(df_name).replace('comparisons_synthetic_pc', '').replace('.p', '')\n",
    "    seed = 30\n",
    "    print('Avg comparisons:', avg_pc)\n",
    "\n",
    "    comparisons_['image_l'] = comparisons_['scene_i']\n",
    "    comparisons_['image_r'] = comparisons_['scene_j']\n",
    "    comparisons_['Winner'] = -1\n",
    "    comparisons_['Loser'] = -1\n",
    "    comparisons_['Tie'] = 0\n",
    "    \n",
    "    for i, row in comparisons_.iterrows():\n",
    "        l_item = row.image_l\n",
    "        r_item = row.image_r\n",
    "        \n",
    "        if row.score == 1:\n",
    "            comparisons_.loc[i, 'Winner'] = r_item\n",
    "            comparisons_.loc[i, 'Loser'] = l_item\n",
    "            comparisons_.loc[i, 'Tie'] = 0\n",
    "        elif row.score == -1:\n",
    "            comparisons_.loc[i, 'Winner'] = l_item\n",
    "            comparisons_.loc[i, 'Loser'] = r_item\n",
    "            comparisons_.loc[i, 'Tie'] = 0\n",
    "        elif row.score == 0:\n",
    "            comparisons_.loc[i, 'Winner'] = r_item\n",
    "            comparisons_.loc[i, 'Loser'] = l_item\n",
    "            comparisons_.loc[i, 'Tie'] = 1\n",
    "\n",
    "    \n",
    "    unique_images = pd.unique(comparisons_[['image_l', 'image_r']].values.ravel('K'))\n",
    "\n",
    "    # Split data in Train, Validation & Test\n",
    "    X_train, X_test = train_test_split(comparisons_, test_size=0.2, random_state=seed)\n",
    "    X_train, X_val  = train_test_split(X_train, test_size=0.13, random_state=seed)\n",
    "    \n",
    "    # Initialize TrueSkill scores\n",
    "    scores = {}\n",
    "\n",
    "    for image in unique_images:\n",
    "        scores[image] = ts.Rating()\n",
    "\n",
    "    # Compute scores based on comparisons\n",
    "    for i, row in X_train.iterrows():\n",
    "        # Define the players in this round\n",
    "        player1 = scores[row['image_l']]\n",
    "        player2 = scores[row['image_r']]\n",
    "        \n",
    "        # Process match\n",
    "        if row['score'] == -1:\n",
    "            score = [0, 1]\n",
    "        elif row['score'] == 0:\n",
    "            score = [0, 0]\n",
    "        elif row['score'] == 1:\n",
    "            score = [1, 0]\n",
    "        \n",
    "        [player1], [player2] = ts.rate([[player1], [player2]], ranks=score)\n",
    "    \n",
    "        # Update scores\n",
    "        scores[row['image_l']] = player1\n",
    "        scores[row['image_r']] = player2\n",
    "\n",
    "    scores_df = pd.DataFrame(scores).T\n",
    "    scores_df.columns = ['score', 'sigma']\n",
    "    scores_df['image_path'] = scores_df.index\n",
    "    scores_df['image'] = scores_df.index\n",
    "    #scores_df['image_path'] = scores_df['image_path'].apply(lambda x: os.path.join('images','berlin', x + '.jpg' ))\n",
    "\n",
    "    # Train\n",
    "    # log_loss_train = compute_logloss(X_train[X_train.score != 0])\n",
    "    accuracy_train = compute_accuracy(X_train[X_train.score != 0])\n",
    "\n",
    "    # Test\n",
    "    # log_loss_test = compute_logloss(X_test[X_test.score != 0])\n",
    "    accuracy_test = compute_accuracy(X_test[X_test.score != 0])\n",
    "\n",
    "    # Compile results\n",
    "    result = {\n",
    "        'avg_pc': avg_pc,\n",
    "        'n_pc': comparisons_.shape[0],\n",
    "        'model': 'trueskill',\n",
    "        'train_accuracy': np.mean(accuracy_train),\n",
    "        'test_accuracy': np.mean(accuracy_test),\n",
    "        'seed': seed\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd5292-a64b-4c08-9b08-0593c612f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_ts = pd.DataFrame(results)\n",
    "results_df_ts.avg_pc = results_df_ts.avg_pc.astype(float)\n",
    "results_df_ts = results_df_ts.sort_values(by=['avg_pc'])\n",
    "results_df_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ba0be-cdb8-4ba3-9e3d-2c6c787c460c",
   "metadata": {},
   "source": [
    "## Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f6b31-4c38-4395-a430-005f469ff1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalELo(object):\n",
    "    def __init__(self, k_factor, elo_width, starting_elo):\n",
    "        self.k_factor = k_factor\n",
    "        self.elo_width = elo_width\n",
    "        self.starting_elo = starting_elo\n",
    "        self.items = set()\n",
    "        self.items_elo = dict()\n",
    "        \n",
    "    def initialize_items(self, items):\n",
    "        \"\"\"Initialize the items available to `items`.\"\"\"\n",
    "        self.items = set(items)\n",
    "    \n",
    "    def initialize_elos(self, ):\n",
    "        \"\"\"Set the initial starting elo for all available items.\"\"\"\n",
    "        for item in self.items:\n",
    "            self.items_elo[item] = self.starting_elo\n",
    "    \n",
    "    def expected_result(self, elo_a, elo_b):\n",
    "        \"\"\"Expected probability of item with elo_a winning vs. item with elo_b.\"\"\"\n",
    "        \n",
    "        expect_a = 1.0/(1+10**((elo_b - elo_a)/self.elo_width))\n",
    "        return expect_a\n",
    "    \n",
    "    def update_elo(self, winner_elo, loser_elo, tie=False):\n",
    "        \"\"\"Update elo for the winning item and losing item.\"\"\"\n",
    "        \n",
    "        R = 1\n",
    "        if tie:\n",
    "            R = .5\n",
    "        \n",
    "        expected_win = self.expected_result(winner_elo, loser_elo)  \n",
    "        change_in_elo = self.k_factor * (R-expected_win)\n",
    "        \n",
    "        winner_elo += change_in_elo\n",
    "        loser_elo -= change_in_elo\n",
    "        return winner_elo, loser_elo\n",
    "    \n",
    "    def add_comparison(self, w_item, l_item, tie=False):\n",
    "        \"\"\"Process comparison between winning item and losing item.\"\"\"\n",
    "        current_winner_elo = self.items_elo[w_item]\n",
    "        current_loser_elo = self.items_elo[l_item]\n",
    "        \n",
    "        updated_winner_elo, updated_loser_elo = self.update_elo(current_winner_elo, current_loser_elo, tie=tie)\n",
    "        \n",
    "        self.items_elo[w_item] = updated_winner_elo\n",
    "        self.items_elo[l_item] = updated_loser_elo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a64526-7523-41f0-946c-7acc16a24858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(elo_a, elo_b, allow_ties=False):\n",
    "    \"\"\"\n",
    "    Expected probabilities of winning, drawing, or losing.\n",
    "    Reference for draws formula: `Mathematical Model of Ranking Accuracy and Popularity Promotion`\n",
    "    https://www.researchgate.net/publication/309662241_Mathematical_Model_of_Ranking_Accuracy_and_Popularity_Promotion\n",
    "    \"\"\"\n",
    "      \n",
    "    p_win = 1. / (1+10**((-elo_a + elo_b)/elo_width))\n",
    "    p_los = 1. / (1+10**((elo_a - elo_b)/elo_width))\n",
    "    \n",
    "    if allow_ties:\n",
    "        p_tie = (1 / (np.sqrt(2 * np.pi) * np.e)) * np.exp(-1 * (( (elo_a-elo_b)/(elo_width/2) )**2) / (2*np.e**2))\n",
    "        p_win = p_win - 0.5 * p_tie  \n",
    "        p_los = p_los - 0.5 * p_tie\n",
    "        \n",
    "        return  p_win, p_los, p_tie\n",
    "\n",
    "    return p_win, p_los\n",
    "    \n",
    "def compute_logloss(df):\n",
    "    log_loss = []\n",
    "    for i, row in df.iterrows():\n",
    "        # p_win, p_los, p_tie = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "        p_win, p_los = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "\n",
    "        if row.score == -1 or row.score == 1:\n",
    "            log_loss.append(np.log(p_win))\n",
    "        else:\n",
    "            log_loss.append(np.log(p_tie))\n",
    "    \n",
    "    return log_loss\n",
    "    \n",
    "def compute_accuracy(df):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        # p_win, p_los, p_tie = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "        p_win, p_los = compute_probabilities(elo.items_elo[row.Winner], elo.items_elo[row.Loser])\n",
    "\n",
    "        if row.score == -1 or row.score == 1:\n",
    "            accuracy.append(int(p_win > p_los))\n",
    "        else:\n",
    "            accuracy.append(int(p_win > p_los))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd77828-65bc-4322-8534-330b57ccc455",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5f01f-7780-4208-9151-4d12e10ff28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = glob('../data/comparisons_synthetic_pc*.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156a68e-6843-4db0-b3f8-d520558f27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for df_name  in model_results:\n",
    "    comparisons_ = pickle.load(open(df_name, 'rb'))\n",
    "    avg_pc = os.path.basename(df_name).replace('comparisons_synthetic_pc', '').replace('.p', '')\n",
    "    seed = 30\n",
    "    print('Avg comparisons:', avg_pc)\n",
    "\n",
    "    comparisons_['image_l'] = comparisons_['scene_i']\n",
    "    comparisons_['image_r'] = comparisons_['scene_j']\n",
    "    comparisons_['Winner'] = -1\n",
    "    comparisons_['Loser'] = -1\n",
    "    comparisons_['Tie'] = 0\n",
    "    \n",
    "    for i, row in comparisons_.iterrows():\n",
    "        l_item = row.image_l\n",
    "        r_item = row.image_r\n",
    "        \n",
    "        if row.score == 1:\n",
    "            comparisons_.loc[i, 'Winner'] = r_item\n",
    "            comparisons_.loc[i, 'Loser'] = l_item\n",
    "            comparisons_.loc[i, 'Tie'] = 0\n",
    "        elif row.score == -1:\n",
    "            comparisons_.loc[i, 'Winner'] = l_item\n",
    "            comparisons_.loc[i, 'Loser'] = r_item\n",
    "            comparisons_.loc[i, 'Tie'] = 0\n",
    "        elif row.score == 0:\n",
    "            comparisons_.loc[i, 'Winner'] = r_item\n",
    "            comparisons_.loc[i, 'Loser'] = l_item\n",
    "            comparisons_.loc[i, 'Tie'] = 1\n",
    "\n",
    "    \n",
    "    unique_images = pd.unique(comparisons_[['image_l', 'image_r']].values.ravel('K'))\n",
    "\n",
    "    # Split data in Train, Validation & Test\n",
    "    X_train, X_test = train_test_split(comparisons_, test_size=0.2, random_state=seed)\n",
    "    X_train, X_val  = train_test_split(X_train, test_size=0.13, random_state=seed)\n",
    "\n",
    "    # ELO\n",
    "    starting_elo = 1500\n",
    "    elo_width = 400\n",
    "    k_factor = 32\n",
    "    \n",
    "    elo = OriginalELo(k_factor=k_factor, \n",
    "                      elo_width=elo_width, \n",
    "                      starting_elo=starting_elo)\n",
    "    elo.initialize_items(list(comparisons_.Winner.values) + list(comparisons_.Loser.values))\n",
    "    elo.initialize_elos()\n",
    "\n",
    "    for i, row in X_train.iterrows():\n",
    "        w_item = row.Winner\n",
    "        l_item = row.Loser \n",
    "        tie = True if row.Tie else False\n",
    "        \n",
    "        elo.add_comparison(w_item, l_item, tie=tie)    \n",
    "    scores = []\n",
    "\n",
    "    for item, item_elo in elo.items_elo.items():\n",
    "        scores.append({\n",
    "            'score': item_elo,\n",
    "            'image': item,\n",
    "            'image_path': os.path.join('images','berlin', item + '.jpg' )\n",
    "        })\n",
    "    scores_df = pd.DataFrame(scores).set_index('image', drop=False)\n",
    "\n",
    "\n",
    "    accuracy_test = compute_accuracy(X_test[X_test.score != 0])\n",
    "    #\n",
    "    # Compile results\n",
    "    result = {\n",
    "        'avg_pc': avg_pc,\n",
    "        'n_pc': comparisons_.shape[0],\n",
    "        'model': 'elo',\n",
    "        'train_accuracy': np.mean(accuracy_train),\n",
    "        'test_accuracy': np.mean(accuracy_test),\n",
    "        'seed': seed\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b1a2d-3d66-46ef-a153-71c6f3e3b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_elo = pd.DataFrame(results)\n",
    "results_df_elo.avg_pc = results_df_elo.avg_pc.astype(float)\n",
    "results_df_elo = results_df_elo.sort_values(by=['avg_pc'])\n",
    "results_df_elo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f74bfd-4b7b-4e0b-b941-165de6aed690",
   "metadata": {},
   "source": [
    "## Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cc83b-d1f1-4934-8534-1bb453369a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kickscore as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86122c91-bb49-405d-a1e8-95133a461c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df, t_):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            p_win, p_tie, p_los = model.probabilities([row.Winner], [row.Loser], t=t_)\n",
    "\n",
    "            if row.score == -1 or row.score == 1:\n",
    "                if p_win > p_los and p_win > p_tie:\n",
    "                    accuracy.append(1) \n",
    "                else:\n",
    "                    accuracy.append(0) \n",
    "            else:  \n",
    "                if p_tie > p_los and p_tie > p_win:\n",
    "                    accuracy.append(1) \n",
    "                else:\n",
    "                    accuracy.append(0) \n",
    "        except KeyError:\n",
    "            continue\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d73230-090c-4a4a-9534-91576c8e05e4",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe4dcc-9252-4e0e-bde4-bd7b6856f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = glob('../data/comparisons_synthetic_pc*.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35f9ef-5744-4881-ac33-6de151132970",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for df_name  in model_results:\n",
    "    comparisons_ = pickle.load(open(df_name, 'rb'))\n",
    "    avg_pc = os.path.basename(df_name).replace('comparisons_synthetic_pc', '').replace('.p', '')\n",
    "    seed = 30\n",
    "    print('Avg comparisons:', avg_pc)\n",
    "\n",
    "    comparisons_['image_l'] = comparisons_['scene_i']\n",
    "    comparisons_['image_r'] = comparisons_['scene_j']\n",
    "    comparisons_['Winner'] = -1\n",
    "    comparisons_['Loser'] = -1\n",
    "    comparisons_['Tie'] = 0\n",
    "    \n",
    "    for i, row in comparisons_.iterrows():\n",
    "        l_item = row.image_l\n",
    "        r_item = row.image_r\n",
    "        \n",
    "        if row.score == 1:\n",
    "            comparisons_.loc[i, 'Winner'] = r_item\n",
    "            comparisons_.loc[i, 'Loser'] = l_item\n",
    "            comparisons_.loc[i, 'Tie'] = 0\n",
    "        elif row.score == -1:\n",
    "            comparisons_.loc[i, 'Winner'] = l_item\n",
    "            comparisons_.loc[i, 'Loser'] = r_item\n",
    "            comparisons_.loc[i, 'Tie'] = 0\n",
    "        elif row.score == 0:\n",
    "            comparisons_.loc[i, 'Winner'] = r_item\n",
    "            comparisons_.loc[i, 'Loser'] = l_item\n",
    "            comparisons_.loc[i, 'Tie'] = 1\n",
    "\n",
    "    \n",
    "    unique_images = pd.unique(comparisons_[['image_l', 'image_r']].values.ravel('K'))\n",
    "\n",
    "    # Split data in Train, Validation & Test\n",
    "    X_train, X_test = train_test_split(comparisons_, test_size=0.2, random_state=seed)\n",
    "    X_train, X_val  = train_test_split(X_train, test_size=0.13, random_state=seed)\n",
    "\n",
    "    # ========================================================================================================== #\n",
    "    # GAUSSIAN PROCESS\n",
    "    images = set()\n",
    "    observations = list()\n",
    "    \n",
    "    for i, row in X_train.reset_index().iterrows():\n",
    "        t = i\n",
    "        images.add(row.image_l)\n",
    "        images.add(row.image_r)\n",
    "        \n",
    "        if row.score == -1:\n",
    "            observations.append({\n",
    "                    'winners': [row.image_l],\n",
    "                    'losers': [row.image_r],\n",
    "                    #'tie': False,\n",
    "                    't': t,\n",
    "                })\n",
    "        if row.score == 0:\n",
    "            observations.append({\n",
    "                    'winners': [row.image_l],\n",
    "                    'losers': [row.image_r],\n",
    "                    'tie': True,\n",
    "                    't': t,\n",
    "                })\n",
    "        if row.score == 1:\n",
    "            observations.append({\n",
    "                    'winners': [row.image_r],\n",
    "                    'losers': [row.image_l],\n",
    "                    #'tie': False,\n",
    "                    't': t,\n",
    "                })\n",
    "\n",
    "    model = ks.TernaryModel(margin=0.2)\n",
    "    kernel = (ks.kernel.Constant(var=0.03))\n",
    "    for image in images:\n",
    "        model.add_item(image, kernel=kernel)\n",
    "    for obs in observations:\n",
    "        model.observe(**obs)\n",
    "\n",
    "    converged = model.fit()\n",
    "    if converged:\n",
    "        print(\"Model has converged.\")\n",
    "\n",
    "    ts = [comparisons_.shape[0] ]  # Point in time at which you want to make the prediction.\n",
    "    res = dict()  # Contains predicted score.\n",
    "    \n",
    "    scores = []\n",
    "    for name, item in model.item.items():\n",
    "        means, var = item.predict(ts)\n",
    "        scores += [[name, means[0], var[0]]]\n",
    "    \n",
    "    scores = pd.DataFrame(scores, columns=['image', 'score', 'var'])\n",
    "    \n",
    "    scores_df = scores.sort_values(by='image').reset_index(drop=True)\n",
    "    #for rank, (name, score) in enumerate(sorted(res.items(), key=lambda x: x[1], reverse=True), start=1):\n",
    "    #    print(f\"rank {rank}: {name} (score: {score:.2f}\")\n",
    "    scores_df = scores_df.set_index('image', drop=False)\n",
    "    scores_df.index.name = None\n",
    "    scores_df['image_path'] = scores_df['image']\n",
    "    scores_df['image_path'] = scores_df['image_path'].apply(lambda x: os.path.join('images','berlin', str(x) + '.jpg' ))\n",
    "\n",
    "    accuracy_train = compute_accuracy(X_train[X_train.score != 0], comparisons_.shape[0])\n",
    "    accuracy_test = compute_accuracy(X_test[X_test.score != 0], comparisons_.shape[0])\n",
    "\n",
    "    # ========================================================================================================== #\n",
    "    \n",
    "    # Compile results\n",
    "    result = {\n",
    "        'avg_pc': avg_pc,\n",
    "        'n_pc': comparisons_.shape[0],\n",
    "        'model': 'gaussian_process',\n",
    "        'train_accuracy': np.mean(accuracy_train),\n",
    "        'test_accuracy': np.mean(accuracy_test),\n",
    "        'seed': seed\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302a1f1-e1b3-4c3a-92eb-fa3893b973da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_gp = pd.DataFrame(results)\n",
    "results_df_gp.avg_pc = results_df_gp.avg_pc.astype(float)\n",
    "results_df_gp = results_df_gp.sort_values(by=['avg_pc'])\n",
    "results_df_gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84fdae3-2c71-4ec1-b158-a50a55cf25c0",
   "metadata": {},
   "source": [
    "## Rank Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008404f-17c4-4286-ac6a-2fedd8efbdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ac1de-f11b-4bfe-8d65-ead32226b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df, params):\n",
    "    accuracy = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row.score == -1:\n",
    "            p_win, p_los = choix.probabilities([int(row.image_l), int(row.image_r)], params)\n",
    "        elif row.score == 1:\n",
    "            p_win, p_los = choix.probabilities([int(row.image_r), int(row.image_l)], params)\n",
    "\n",
    "        if row.score == -1 or row.score == 1:\n",
    "            if p_win > p_los:\n",
    "                accuracy.append(1) \n",
    "            else:\n",
    "                accuracy.append(0)  \n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a626b6-f152-4244-bcb9-f9053db5a4d3",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e367fe-872a-4a3d-b9c9-6720815accc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = glob('../data/comparisons_synthetic_pc*.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeaf7e0-b7a2-4326-ac45-8a886aac9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for df_name in model_results:\n",
    "    comparisons_ = pickle.load(open(df_name, 'rb'))\n",
    "    avg_pc = os.path.basename(df_name).replace('comparisons_synthetic_pc', '').replace('.p', '')\n",
    "    seed = 30\n",
    "    print('Avg comparisons:', avg_pc)\n",
    "\n",
    "    comparisons_['image_l'] = comparisons_['scene_i']\n",
    "    comparisons_['image_r'] = comparisons_['scene_j']\n",
    "    comparisons_['Winner'] = -1\n",
    "    comparisons_['Loser'] = -1\n",
    "    comparisons_['Tie'] = 0\n",
    "    \n",
    "    for i, row in comparisons_.iterrows():\n",
    "        l_item = row.image_l\n",
    "        r_item = row.image_r\n",
    "        \n",
    "        if row.score == 1:\n",
    "            comparisons_.loc[i, 'Winner'] = r_item\n",
    "            comparisons_.loc[i, 'Loser'] = l_item\n",
    "            comparisons_.loc[i, 'Tie'] = 0\n",
    "        elif row.score == -1:\n",
    "            comparisons_.loc[i, 'Winner'] = l_item\n",
    "            comparisons_.loc[i, 'Loser'] = r_item\n",
    "            comparisons_.loc[i, 'Tie'] = 0\n",
    "        elif row.score == 0:\n",
    "            comparisons_.loc[i, 'Winner'] = r_item\n",
    "            comparisons_.loc[i, 'Loser'] = l_item\n",
    "            comparisons_.loc[i, 'Tie'] = 1\n",
    "\n",
    "    # ========================================================================================================== #\n",
    "    # RANK CENTRALITY\n",
    "    n_items = len(pd.unique(comparisons_[['image_l', 'image_r']].values.ravel('K')))\n",
    "    images = pd.unique(comparisons_[['image_l', 'image_r']].values.ravel('K'))\n",
    "    images_dict = {}\n",
    "    images_dict_rev = {}\n",
    "    for i, image_id in enumerate(images):\n",
    "        images_dict[image_id] = i\n",
    "        images_dict_rev[i] = image_id\n",
    "    comparisons_=comparisons_.replace({\"image_l\": images_dict})\n",
    "    comparisons_=comparisons_.replace({\"image_r\": images_dict})\n",
    "    comparisons_=comparisons_.replace({\"Winner\": images_dict})\n",
    "    comparisons_=comparisons_.replace({\"Loser\": images_dict})\n",
    "\n",
    "    # Split data in Train, Validation & Test\n",
    "    X_train, X_test = train_test_split(comparisons_, test_size=0.2, random_state=seed)\n",
    "    X_train, X_val  = train_test_split(X_train, test_size=0.13, random_state=seed)\n",
    "\n",
    "    data = []\n",
    "    for i, row in X_train.iterrows():\n",
    "        if not row.Tie:\n",
    "            data.append((int(row.Winner), int(row.Loser)))\n",
    "    \n",
    "        if row.Tie:\n",
    "            data.append((row.Winner, row.Loser))\n",
    "            data.append((row.Loser, row.Winner))\n",
    "    \n",
    "    params_rc = choix.rank_centrality(n_items, data, alpha=1e-4)\n",
    "    \n",
    "    accuracy_train = compute_accuracy(X_train[X_train.score != 0], params_rc)\n",
    "    accuracy_test = compute_accuracy(X_test[X_test.score != 0], params_rc)\n",
    "    \n",
    "    # ========================================================================================================== #\n",
    "    \n",
    "    # Compile results\n",
    "    result = {\n",
    "        'avg_pc': avg_pc,\n",
    "        'n_pc': comparisons_.shape[0],\n",
    "        'model': 'rank_centrality',\n",
    "        'train_accuracy': np.mean(accuracy_train),\n",
    "        'test_accuracy': np.mean(accuracy_test),\n",
    "        'seed': seed\n",
    "    }\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b2b90-aa03-4b2c-9c70-b9e492e8480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_rc = pd.DataFrame(results)\n",
    "results_df_rc.avg_pc = results_df_rc.avg_pc.astype(float)\n",
    "results_df_rc = results_df_rc.sort_values(by=['avg_pc'])\n",
    "results_df_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4bb174-70ae-4091-a9e1-f21f2bf4e2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a909510-8e7b-48dc-8891-2ff4f73cd27c",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797aa895-8752-4656-b7c7-7d7966615094",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "#plt.plot(df.avg_pc, df.ranking_acc, 'k', label='With ties')\n",
    "plt.plot(results_df_pcs.avg_pc, results_df_pcs.ranking_acc_nonties_nomargin, 'b', label='PCS-Net', )\n",
    "plt.plot(results_df_ts.avg_pc, results_df_ts.test_accuracy, 'g:', label='TrueSkill', )\n",
    "plt.plot(results_df_elo.avg_pc, results_df_elo.test_accuracy, 'r:', label='Elo', )\n",
    "plt.plot(results_df_gp.avg_pc, results_df_gp.test_accuracy, 'c:', label='Gaussian Process', )\n",
    "plt.plot(results_df_rc.avg_pc, results_df_rc.test_accuracy, 'y:', label='Rank Centrality', )\n",
    "plt.xlabel('Avg. Comparisons per Image')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0.5, 1)\n",
    "plt.xlim(0, 40)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7656d9c-589d-472e-b5ef-d5bc773b17fd",
   "metadata": {},
   "source": [
    "# PCS-Net: Ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14fb54-fcb3-4366-9a06-7019a46b0f4a",
   "metadata": {},
   "source": [
    "## PCS-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb7307-49a7-45b9-b30e-8df4bc0c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_accuracy(df_, margin=0):\n",
    "    # Split in non-ties and ties\n",
    "    df_nonties = df_[df_.label_r != 0]\n",
    "    df_ties = df_[df_.label_r == 0]\n",
    "\n",
    "    # Split non ties per their outcome (left and right)\n",
    "    df_nonties_left = df_nonties[(df_nonties.label_r == -1)]\n",
    "    df_nonties_right = df_nonties[(df_nonties.label_r == 1)]\n",
    "\n",
    "    # Non-ties accuracy\n",
    "    correct_left = ((df_nonties.label_r == -1) & (df_nonties.rank_left - df_nonties.rank_right > margin)).sum()\n",
    "    correct_right = ((df_nonties.label_r == 1) & (df_nonties.rank_right - df_nonties.rank_left > margin)).sum()\n",
    "\n",
    "    nontie_left_accuracy = correct_left / (df_nonties.label_r == -1).sum()\n",
    "    nontie_right_accuracy = correct_right / (df_nonties.label_r == 1).sum()\n",
    "    nontie_accuracy = (correct_left + correct_right ) / df_nonties.shape[0]\n",
    "    \n",
    "    # Ties accuracy\n",
    "    tie_accuracy = (abs(df_ties.rank_left - df_ties.rank_right) < margin).sum() / df_ties.shape[0]\n",
    "\n",
    "    # Overall accuracy\n",
    "    overall_accuracy = X_test[((df_.label_r == -1) & (df_.rank_left - df_.rank_right > margin)) |\n",
    "                              ((df_.label_r ==  1) & (df_.rank_right - df_.rank_left > margin)) |\n",
    "                              ((df_.label_r ==  0) & (abs(df_.rank_left - df_.rank_right) < margin))].shape[0] / df_.shape[0]\n",
    "    \n",
    "    return nontie_left_accuracy, nontie_right_accuracy, nontie_accuracy, tie_accuracy, overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30b6ce-b764-430e-a974-80b5a5100d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_accuracy_nomargin(df_,):\n",
    "    # Split in non-ties and ties\n",
    "    df_nonties = df_[df_.label_r != 0]\n",
    "\n",
    "    # Split non ties per their outcome (left and right)\n",
    "    df_nonties_left = df_nonties[(df_nonties.label_r == -1)]\n",
    "    df_nonties_right = df_nonties[(df_nonties.label_r == 1)]\n",
    "\n",
    "    # Non-ties accuracy\n",
    "    correct_left = ((df_nonties.label_r == -1) & (df_nonties.rank_left - df_nonties.rank_right > 0)).sum()\n",
    "    correct_right = ((df_nonties.label_r == 1) & (df_nonties.rank_right - df_nonties.rank_left > 0)).sum()\n",
    "\n",
    "    nontie_left_accuracy = correct_left / (df_nonties.label_r == -1).sum()\n",
    "    nontie_right_accuracy = correct_right / (df_nonties.label_r == 1).sum()\n",
    "    nontie_accuracy = (correct_left + correct_right ) / df_nonties.shape[0]\n",
    "\n",
    "    return nontie_left_accuracy, nontie_right_accuracy, nontie_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14828271-421b-4f0e-b2ad-3c4ae579ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_distance(df_):\n",
    "    # Split in non-ties and ties\n",
    "    df_nonties = df_[df_.label_r != 0]\n",
    "    df_ties = df_[df_.label_r == 0]\n",
    "\n",
    "    # Distance between non-ties\n",
    "    avg_dist_nonties = abs(df_nonties.rank_left - df_nonties.rank_right).mean()\n",
    "    \n",
    "    # Distance between ties\n",
    "    avg_dist_ties = abs(df_ties.rank_left - df_ties.rank_right).mean()\n",
    "    \n",
    "    return avg_dist_nonties, avg_dist_ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0465c-ba06-400f-82a1-9f1e8645cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy(df_):\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "        \n",
    "    col1_values = df_['logits_l'].values\n",
    "    col2_values = df_['logits_0'].values\n",
    "    col3_values = df_['logits_r'].values\n",
    "        \n",
    "    probabilities = np.apply_along_axis(softmax, axis=1, arr=np.column_stack((col1_values, col2_values, col3_values)))\n",
    "    max_indices = np.argmax(probabilities, axis=1)\n",
    "    # Convert the probabilities back to a DataFrame with appropriate column names\n",
    "    softmax_df = pd.DataFrame(probabilities, columns=['softmax_logit_l', 'softmax_logit_0', 'softmax_logit_r'])\n",
    "    max_index_df = pd.DataFrame({'class_predicted': max_indices})\n",
    "    # Concatenate the new DataFrame with the original DataFrame if needed\n",
    "    result_df = pd.concat([df.reset_index(drop=True), softmax_df, max_index_df], axis=1,)\n",
    "\n",
    "    all_accuracy = (result_df.class_predicted == result_df.label_c).sum() / result_df.shape[0]\n",
    "    tie_accuracy = (result_df[result_df.label_c == 1].class_predicted == result_df[result_df.label_c == 1].label_c).sum() / result_df.shape[0]\n",
    "    nontie_accuracy = (result_df[result_df.label_c != 1].class_predicted == result_df[result_df.label_c != 1].label_c).sum() / result_df.shape[0]\n",
    "    \n",
    "    return all_accuracy, tie_accuracy, nontie_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960f671-b960-4995-90e5-44ef2dec13a4",
   "metadata": {},
   "source": [
    "### Available results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348778e-6dc7-43d0-abf8-6d74c35cbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = glob('../outputs/saved/synthetic-*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcf384-0564-4668-b134-1feb94d86c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = glob('../data/comparisons_synthetic_pc*.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152f7b4-83d1-4f3b-b461-6b2294430220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1e454-fbea-493e-bc2f-82d56e6822c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(model_results[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed93848-7d92-4a74-827b-fcf036f26a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_0 = df[df.rank_right == df.rank_right.min()].image_right.iloc[0]\n",
    "rank_1 = '/home/mncosta/data/images/01_MS_C_901.jpg'\n",
    "rank_2 = '/home/mncosta/data/images/01_CP_C_828.jpg'\n",
    "rank_3 = df[df.rank_right == df.rank_right.max()].image_right.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa491f-5ea2-497c-96d9-b5bee87b1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(1,4, figsize=(15, 15));\n",
    "axarr[0].imshow(mpimg.imread(rank_0))\n",
    "axarr[0].text(50, 900, '-1.83', style='italic', color='w', fontsize='large', bbox={'facecolor': 'grey', 'alpha': 0.9, 'pad': 1})\n",
    "axarr[0].set_xticks([]);\n",
    "axarr[0].set_yticks([]);\n",
    "axarr[1].imshow(mpimg.imread(rank_1))\n",
    "axarr[1].text(50, 900, '-1.06', style='italic', color='w', fontsize='large', bbox={'facecolor': 'grey', 'alpha': 0.9, 'pad': 1})\n",
    "axarr[1].set_xticks([]);\n",
    "axarr[1].set_yticks([]);\n",
    "axarr[2].imshow(mpimg.imread(rank_2))\n",
    "axarr[2].text(50, 900, '0.82', style='italic', color='w', fontsize='large', bbox={'facecolor': 'grey', 'alpha': 0.9, 'pad': 1})\n",
    "axarr[2].set_xticks([]);\n",
    "axarr[2].set_yticks([]);\n",
    "axarr[3].imshow(mpimg.imread(rank_3))\n",
    "axarr[3].text(50, 900, '1.38', style='italic', color='w', fontsize='large', bbox={'facecolor': 'grey', 'alpha': 0.9, 'pad': 1})\n",
    "axarr[3].set_xticks([]);\n",
    "axarr[3].set_yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ccaa09-d1eb-4702-a9ec-4772f93e26ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f2f77-7bf8-4b55-bc22-3be35d01fac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5b0a7-0932-4ad3-bf34-aa3c10d05129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3f4c6-ab43-445b-83bc-d47982da9624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed1b6e-c6be-4a42-8ede-d52898a61d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
